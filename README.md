# ğŸŒ± Greenhouse Machineâ€‘Learning Service (`mal`)

This repository contains the **machineâ€‘learning microâ€‘service** for the SEP4 Greenhouse project.  It trains and serves lightweight ML models that decide
when to **water the plants** and when to **open the ventilation** based on telemetry streamed from the IoT node (`ews`).  The service is designed for lowâ€‘latency inference on AWS Lambda while retaining reproducible, versionâ€‘controlled builds via GitHub Actions and Terraform.

> **Key facts**
>
> | Â         | Details                                                        |
> | -------- | -------------------------------------------------------------- |
> | Language | PythonÂ 3.11                                                    |
> | Runtime  | AWSÂ Lambda (zip + layers)                                      |
> | MLÂ libs  | *scikitâ€‘learn*, *pandas*, *numpy*                              |
> | CI/CD    | GitHub ActionsÂ â†’ Release assetsÂ â†’ Terraform deploy             |
> | Infra    | S3 (telemetry+artifacts), APIÂ Gateway, EventBridge, CloudWatch |

---

## TableÂ ofÂ Contents

1. [Architecture](#architecture)
2. [RepositoryÂ Layout](#repository-layout)
3. [QuickÂ Start](#quick-start)
4. [DataÂ Schema](#data-schema)
5. [TrainingÂ &Â InferenceÂ Flows](#training--inference-flows)
6. [Configuration](#configuration)
7. [DevOpsÂ Pipeline](#devops-pipeline)
8. [Testing](#testing)
9. [Contributing](#contributing)

---

## Architecture

```mermaid
flowchart TD
    A[IoTÂ node (AVR\n`ews`)] -- CSV telemetry --> S3[(Telemetry bucket)]
    subgraph ML Service
        B(APIÂ Gateway \n POST /v1/predict)
        C[Lambda handler (src/handler.py)]
        D[(ML models\n.joblib in S3)]
    end
    B --> C
    C -- get/put --> D
    E[GitHub ActionsÂ CD] --> F((Release assets)) --> Terraform --> C
    EventBridge -->|hourly| C
```

* **Inference**: The AVR device POSTs a JSON event to **/v1/predict**; APIÂ Gateway invokes the Lambda which loads the latest model from S3 and returns aÂ binary decision (on/off) plus probability.
* **Training**: An hourly EventBridge trigger (or manual API call) makes the Lambda download the most recent CSV telemetry, fit a RandomÂ Forest model for each target *(water, vent)*, and write the model + metrics back to S3.

---

## RepositoryÂ Layout

```
mal/
â”œâ”€â”€ .github/workflows/       # CI/CD pipeline (build layers + release)
â”‚Â Â  â””â”€â”€ cd.yml
â”œâ”€â”€ scripts/                 # build helpers
â”‚Â Â  â”œâ”€â”€ build_layers.sh      # ğŸÂ numpy/pandas/sklearn â†’ layer zips
â”‚Â Â  â”œâ”€â”€ build_zip.sh         # handler â†’ ml_service.zip
â”‚Â Â  â””â”€â”€ publish_artifact.sh  # optional S3 + Lambda update
â”œâ”€â”€ src/
â”‚Â Â  â””â”€â”€ handler.py           # Lambda entryâ€‘point (train / predict / ping)
â”œâ”€â”€ requirements.txt         # runtime deps (mirrors build_layers versions)
â””â”€â”€ README.md                # you are here ğŸ™Œ
```

---

## QuickÂ Start

### 1. Local dev (venv)

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
python -m pytest   # run unit tests (WIP)
```

### 2. Build artefacts manually

```bash
./scripts/build_layers.sh   # creates .lambda_layers/{sk1,sk2}.zip
./scripts/build_zip.sh      # creates build/ml_service.zip
```

### 3. Invoke the handler locally

Use **dockerâ€‘lambda** or **AWSÂ SAM CLI**:

```bash
sam local invoke -e events/predict_water.json \
  --env-vars sam.env.json
```

### 4. Deploy (CIâ€‘driven)

1. Push to `main` â†’ **cd.yml** builds layers + zip and publishes a GitHubÂ Release.
2. The **terraform** repo downloads those release assets and provisions layers, Lambda, EventBridgeÂ rule, and the public HTTP endpoint.

> â„¹ï¸Â All AWS resources live in the `greenhouse-ml` Terraform module for ease of reuse.

---

## DataÂ Schema

| Column        | Type     | Description                     |
| ------------- | -------- | ------------------------------- |
| `DeviceMac`   | string   | MAC address of IoT node         |
| `Timestamp`   | ISOÂ 8601 | Sample time (UTC)               |
| `Temperature` | Â°C       | Air temperature                 |
| `Humidity`    | %Â RH     | Air humidity                    |
| `Soil`        | %        | Soil moisture (0Â dryÂ â€“Â 100Â wet) |
| `Lux`         | lux      | Light intensity                 |
| `Level`       | cm       | Tank water level                |
| `Motion`      | 0/1      | PIR motion flag                 |
| `Tamper`      | 0/1      | ADXL345 tamper flag             |
| `AccelX/Y/Z`  | int      | Raw accelerometer axes          |
| `MlWater`     | 0/1      | Groundâ€‘truth label (optional)   |
| `MlVent`      | 0/1      | Groundâ€‘truth label (optional)   |

If labels are missing the Lambda derives **heuristic labels** (waterÂ â†”Â `SoilÂ <=Â 40`, ventÂ â†”Â `HumidityÂ <=Â 45`) so it can train even with unlabeled data.

---

## TrainingÂ &Â InferenceÂ Flows

### Train

```jsonc
{
  "action": "train",
  "greenhouse_id": "gh-001",
  "s3_uri": "s3://<bucket>/ml/training.csv", // OR bucket+key
  "target": "water" // water|vent|omitâ†’both
}
```

* **Model**: `RandomForestClassifier(n_estimators=150, class_weight="balanced_subsample")`
* **Outputs**:Â `ml/models/<gh>/<target>_rf_cls.joblib` + metrics JSON under `ml/metrics/â€¦`.

### Predict

```jsonc
{
  "action": "predict",
  "greenhouse_id": "gh-001",
  "target": "vent",
  "features": {
    "Temperature": 25,
    "Humidity": 50,
    "Soil": 55,
    "Lux": 1200,
    "Level": 18,
    "Motion": 0,
    "Tamper": 0,
    "AccelX": 8,
    "AccelY": -3,
    "AccelZ": 1023,
    "Hour": 14
  }
}
```

Response:

```jsonc
{
  "ok": true,
  "result": {
    "target": "vent",
    "prediction": 0,
    "probability": 0.27
  }
}
```

---

## Configuration

| ENVÂ var            | Default   | Purpose                        |
| ------------------ | --------- | ------------------------------ |
| `S3_BUCKET`        | *(none)*  | Telemetry & model bucket       |
| `MIN_SAMPLES`      | `10`      | Minimum rows required to train |
| `AWS_ENDPOINT_URL` | *(unset)* | Override for **LocalStack**    |

---

## DevOpsÂ Pipeline

### GitHubÂ Actions â€“Â `.github/workflows/cd.yml`

1. **Checkout**
2. **SetÂ upÂ PythonÂ 3.11**
3. **CacheÂ pip** deps
4. **BuildÂ Lambda layers** (`build_layers.sh`)
5. **BuildÂ handlerÂ ZIP** (`build_zip.sh`)
6. **Create GitHubÂ Release** with assets
7. *(Optional)* Publish artifacts directly to S3 + update Lambda

### Terraform (sibling repo)

* Downloads release assets â†’ `local_file` resources
* Creates **Lambda layers** & **function**
* Exposes **/v1/predict** via APIÂ Gateway (HTTPÂ API)
* Schedules hourly retraining via EventBridge

---


For integration you can spin up **LocalStack**:

```bash
localstack start -d
export AWS_ENDPOINT_URL=http://localhost:4566
python src/handler.py  # invoke however you like
```

---

## Contributing

1. Fork & create a feature branch.
2. Follow the [commitÂ convention](https://www.conventionalcommits.org/) â€“ this feeds the release notes.
3. Run `preâ€‘commit run â€‘â€‘all-files` before pushing.
4. Open a PR â€“ CI must be âœ”ï¸ for merge.

---

## License

Distributed under the MITÂ License.  See `LICENSE` for details.

---

> *SEP4 â€” 2025 â€¢ VIAÂ UniversityÂ College*
